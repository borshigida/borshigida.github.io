<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Boris Shigida </title> <meta name="author" content="Boris Shigida"> <meta name="description" content=""> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%99%83&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://borshigida.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="/assets/pdf/cv.pdf" title="CV" target="_blank"><i class="ai ai-cv"></i></a> <a href="mailto:%62%73%31%36%32%34@%70%72%69%6E%63%65%74%6F%6E.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/boris-shigida" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=KImeZo8AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Boris</span> Shigida </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?b32d9d4f994b8db514c1b8cf6d5f0aa2" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>221 Sherrerd Hall</p> <p>Charlton Street</p> <p>Princeton, NJ 08544</p> </div> </div> <div class="clearfix"> <p>I am a PhD student at <a href="https://orfe.princeton.edu/" rel="external nofollow noopener" target="_blank">Princeton ORFE</a> working on theoretical and empirical deep learning and mathematical statistics. I am privileged to be advised by <a href="https://cattaneo.princeton.edu/" rel="external nofollow noopener" target="_blank">Matias D. Cattaneo</a> and <a href="https://boris-hanin.github.io/" rel="external nofollow noopener" target="_blank">Boris Hanin</a>. Prior to Princeton, I studied mathematics (specializing in probability theory) at Moscow State University in Moscow where I grew up, and worked part-time as a software engineer at Yandex (Russian search company). I have enjoyed both writing code / keeping track of experiments and proving theorems.</p> </div> <h2 style="color: inherit">Papers</h2> All authors are listed alphabetically. <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arxiv</abbr> </div> <div id="cattaneo2025modifiedlossmomentum" class="col-sm-8"> <div class="title">Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis</div> <div class="author"> Matias D. Cattaneo and Boris Shigida </div> <div class="periodical"> <em>arXiv preprint arxiv:2509.08483</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2509.08483" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2509.08483v1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>We analyze gradient descent with heavy-ball momentum (HB) whose fixed momentum parameter \(\beta \in (0, 1)\) provides exponential decay of memory. Building on Kovachki and Stuart (2021), we prove that on an exponentially attractive invariant manifold the algorithm is exactly plain gradient descent with a modified loss, provided that the step size \(h\) is small enough. Although the modified loss does not admit a closed-form expression, we describe it with arbitrary precision and prove global (finite ``time'' horizon) approximation bounds \(O(h^{R})\) for any finite order \(R \geq 2\). We then conduct a fine-grained analysis of the combinatorics underlying the memoryless approximations of HB, in particular, finding a rich family of polynomials in \(\beta\) hidden inside which contains Eulerian and Narayana polynomials. We derive continuous modified equations of arbitrary approximation order (with rigorous bounds) and the principal flow that approximates the HB dynamics, generalizing Rosca et al. (2023). Approximation theorems cover both full-batch and mini-batch HB. Our theoretical results shed new light on the main features of gradient descent with heavy-ball momentum, and outline a road-map for similar analysis of other optimization algorithms.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cattaneo2025modifiedlossmomentum</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cattaneo, Matias D. and Shigida, Boris}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arxiv:2509.08483}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2509.08483}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">neurips</abbr> </div> <div id="cattaneo2025howmemory" class="col-sm-8"> <div class="title">How Memory in Optimization Algorithms Implicitly Modifies the Loss</div> <div class="author"> Matias D. Cattaneo and Boris Shigida </div> <div class="periodical"> <em>In The Thirty-ninth Annual Conference on Neural Information Processing Systems</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=2qd4lpXz7u" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2502.02132v2.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In modern optimization methods used in deep learning, each update depends on the history of previous iterations, often referred to as <i>memory</i>, and this dependence decays fast as the iterates go further into the past. For example, gradient descent with momentum has exponentially decaying memory through exponentially averaged past gradients. We introduce a general technique for identifying a memoryless algorithm that approximates an optimization algorithm with memory. It is obtained by replacing all past iterates in the update by the current one, and then adding a correction term arising from memory (also a function of the current iterate). This correction term can be interpreted as a perturbation of the loss, and the nature of this perturbation can inform how memory implicitly (anti-)regularizes the optimization dynamics. As an application of our theory, we find that Lion does not have the kind of implicit anti-regularization induced by memory that AdamW does, providing a theory-based explanation for Lion's better generalization performance recently documented. Empirical evaluations confirm our theoretical findings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cattaneo2025howmemory</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{How Memory in Optimization Algorithms Implicitly Modifies the Loss}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cattaneo, Matias D. and Shigida, Boris}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{The Thirty-ninth Annual Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=2qd4lpXz7u}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">arxiv</abbr> </div> <div id="cattaneo2025uniformestimation" class="col-sm-8"> <div class="title">Uniform Estimation and Inference for Nonparametric Partitioning-Based M-Estimators</div> <div class="author"> Matias D. Cattaneo, Yingjie Feng, and Boris Shigida </div> <div class="periodical"> <em>Under revision: Annals of Statistics</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2409.05715" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2409.05715v2.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper presents uniform estimation and inference theory for a large class of nonparametric partitioning-based M-estimators. The main theoretical results include: (i) uniform consistency for convex and non-convex objective functions; (ii) rate-optimal uniform Bahadur representations; (iii) rate-optimal uniform (and mean square) convergence rates; (iv) valid strong approximations and feasible uniform inference methods; and (v) extensions to functional transformations of underlying estimators. Uniformity is established over both the evaluation point of the nonparametric functional parameter and a Euclidean parameter indexing the class of loss functions. The results also account explicitly for the smoothness degree of the loss function (if any), and allow for a possibly non-identity (inverse) link function. We illustrate the theoretical and methodological results in four examples: quantile regression, distribution regression, \(L_p\)-regression, and Logistic regression. Many other possibly non-smooth, nonlinear, generalized, robust M-estimation settings are covered by our results. We provide detailed comparisons with the existing literature and demonstrate substantive improvements: we achieve the best (in some cases optimal) known results under improved (in some cases minimal) requirements in terms of regularity conditions and side rate restrictions. The supplemental appendix reports complementary technical results that may be of independent interest, including a novel uniform strong approximation result based on Yurinskii's coupling.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">cattaneo2025uniformestimation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Uniform Estimation and Inference for Nonparametric Partitioning-Based M-Estimators}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cattaneo, Matias D. and Feng, Yingjie and Shigida, Boris}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Under revision: Annals of Statistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2409.05715}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">icml</abbr> </div> <div id="cattaneo2024onimplicitbias" class="col-sm-8"> <div class="title">On the Implicit Bias of Adam</div> <div class="author"> Matias D. Cattaneo, Jason M. Klusowski, and Boris Shigida </div> <div class="periodical"> <em>In Forty-first International Conference on Machine Learning</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openreview.net/forum?id=y8YovS0lOg" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/2309.00079v4.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different ``norm'' involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, conversely, impede its reduction (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cattaneo2024onimplicitbias</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Implicit Bias of Adam}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cattaneo, Matias D. and Klusowski, Jason M. and Shigida, Boris}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Forty-first International Conference on Machine Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=y8YovS0lOg}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">mcap</abbr> </div> <div id="bulinsk2021discretetime" class="col-sm-8"> <div class="title">Discrete-time model of company capital dynamics with investment of a certain part of surplus in a non-risky asset for a fixed period</div> <div class="author"> Ekaterina V. Bulinskaya and Boris Shigida </div> <div class="periodical"> <em>Methodology and Computing in Applied Probability</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s11009-020-09843-5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>A periodic-review insurance model is studied under the following assumptions. One-period insurance claims form a sequence of independent identically distributed nonnegative random variables with a finite mean. At the beginning of each period a quota δ of the company surplus is invested in a non-risky asset for m periods. Theoretical expressions for finite-time and ultimate ruin probabilities, in terms of multiple integrals, are presented and applied to the particular case where claims are exponential. Dividend problems are also considered. Numerical results obtained by virtue of simulation are provided and other algorithmic approaches are discussed. Sensitivity analysis of ruin probability is carried out for the case of exponential claims.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bulinsk2021discretetime</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Discrete-time model of company capital dynamics with investment of a certain part of surplus in a non-risky asset for a fixed period}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bulinskaya, Ekaterina V. and Shigida, Boris}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Methodology and Computing in Applied Probability}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://link.springer.com/article/10.1007/s11009-020-09843-5}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">jms</abbr> </div> <div id="bulinsk2019sensitivity" class="col-sm-8"> <div class="title">Sensitivity Analysis of Some Applied Probability Models</div> <div class="author"> Ekaterina V. Bulinskaya and Boris Shigida </div> <div class="periodical"> <em>English version: Journal of Mathematical Sciences</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s10958-021-05318-1" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>During the last two decades, new models were developed in actuarial sciences. Different notions of insurance company ruin (bankruptcy) and other objective functions evaluating the company performance were introduced. Several types of decision (such as dividend payment, reinsurance, investment) are used for optimization of company functioning. Therefore, it is necessary to be sure that the model under consideration is stable with respect to parameter fluctuation and perturbation of underlying stochastic processes. The aim of this paper is the description of methods for investigation of these problems and presentation of recent results concerning some insurance models. Numerical results are also included.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bulinsk2019sensitivity</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sensitivity Analysis of Some Applied Probability Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bulinskaya, Ekaterina V. and Shigida, Boris}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{English version: Journal of Mathematical Sciences}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://link.springer.com/article/10.1007/s10958-021-05318-1}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">commstat</abbr> </div> <div id="bulinsk2019modelingandasymp" class="col-sm-8"> <div class="title">Modeling and asymptotic analysis of insurance company performance</div> <div class="author"> Ekaterina V. Bulinskaya and Boris Shigida </div> <div class="periodical"> <em>Communications in Statistics Part B: Simulation and Computation</em>, 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.tandfonline.com/doi/abs/10.1080/03610918.2019.1612911" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="abstract hidden"> <p>We consider a classical Cramér-Lundberg model with dividends. It is additionally supposed that the claim amounts have exponential distribution. Moreover, we are interested in a barrier dividend strategy with Parisian implementation delay. That means, the payment is made only if the company surplus stays above the barrier at least during time interval of length h. The mean expected discounted dividends paid before Parisian ruin are chosen as objective function. Optimization is carried out. The results are compared with those obtained previously by the authors for the no-delay case. Statistical estimation, stability problems and simulation are tackled as well.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bulinsk2019modelingandasymp</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modeling and asymptotic analysis of insurance company performance}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bulinskaya, Ekaterina V. and Shigida, Boris}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Communications in Statistics Part B: Simulation and Computation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">abs</span> <span class="p">=</span> <span class="s">{We consider a classical Cramér-Lundberg model with dividends. It is additionally supposed that the claim amounts have exponential distribution. Moreover, we are interested in a barrier dividend strategy with Parisian implementation delay. That means, the payment is made only if the company surplus stays above the barrier at least during time interval of length h. The mean expected discounted dividends paid before Parisian ruin are chosen as objective function. Optimization is carried out. The results are compared with those obtained previously by the authors for the no-delay case. Statistical estimation, stability problems and simulation are tackled as well.}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.tandfonline.com/doi/abs/10.1080/03610918.2019.1612911}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Boris Shigida. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>